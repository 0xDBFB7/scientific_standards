
To the public, anything in a pdf format and with big words is now science, even if it completely defies every best practice. Poor-quality science will undermine itself.

You lose the rights to contributed content - will that work?




Selected documents

it is hoped that eventually this might be machine-readable and integrated with hargrave in some fashion, 
if that ever becomes more than a pipe dream.






https://en.wikipedia.org/wiki/Joan_Curran

meta_standard (for adding new standards)


Sections:


describe also the context and position in terms of the literature; superceding techniques,


"Successful" uses 
"Unsuccessful" uses 

Reviewed by:

Daniel [non-expert, unverified]

Permission to contact (in accordance with the guidelines): 


"Positive" benchmark

"Negative" benchmark

Probably it will become dormant in a few weeks and nothing will matter.

Simple, orthogonal tests for all reagents or supplies that can be performed by 
as many people as possible to put everything into a "known state";
regular audits?


Must track progress as rapidly as possible. 
There are few central clearinghouses for the state-of-the-art!



It might end up being a total waste of time!


Books regarding techniques are probably also good ways to get the same knowledge.


When adding, you can agree to be added to a mailing list. If changes are requested, you 
may be contacted to obtain approval.


unit and integration tests for science

On mailing list and identies


On the other hand, a list of the contact information of every expert in the field seems 
 rife for abuse



of course, science lies on a wide distribution from true experts producing unassailable results
to poor-quality. Judging based on the bottom tail gives a 


best practices are often buried in decades of 

if you want a good result, this is how you do it.

Paper writing itself:

orcid, 

(of course, standards themselves might proliferate!)



for materials science, this might be a certain test coupon.
For batteries, a very specific technical performance metric across chemistries.


To obtain a certain hardness metric (say, Rockwell), a certain number of procedures are dictated to obtain 
a consistent result. 


"best available" experiment design for the job.

SHOULD Design Guideline for Microfluidic Device and
Component Interfaces




"In particular, they MUST only be used where it is
   actually required for interoperation or to limit behavior which has
   potential for causing harm (e.g., limiting retransmisssions)  For
   example, they must not be used to try to impose a particular method
   on implementors where the method is not required for
   interoperability."
   




 and, indeed, monumental people are 

TEM laser paper

However, a cursory or Scientific Reports produces a most depressing opposition.
The accuracy very probably remained unchanged, but the volume ; and so the literature has devolved into a kind of blur;
it no longer offers any guidance on what problems to attack, which directions are fruitful.

reduce down to the minimum requirements and most easily implementable; to drive towards
greater "stability".

of course, judging by the most infamous examples doesn't do justice to the best science.
Still, 




 
These are fundamental parts of science, but not everyon




tech-transfer.



add biological microwave spectrometry 

add Pakhomov standard waveguide
add "data" section

add mosquito repellant test standard culture
add laser breakdown "comment on" article

cite microfluidic standard

At bottom of standard, document changelog with previous deficiencies
benchmarks - there must be a way to justifiably transfer to new equipment or update a standard.
I guess that means you have to repeat one or more of the most demanding previous positive and negative
tests and ensure that you 



automatically move sections into checklist and lit. review
https://unix.stackexchange.com/questions/180663/how-to-select-first-occurrence-between-two-patterns-including-them?noredirect=1&lq=1




One-tailed or two-tailed test?


.standard.md
.culture.py

RATIONALE

PERSPECTIVE [DC]



ab initio if possible (relying as little as possible on specific manufacturers, reagents, etc),
 which is easy to say but hard to implement in practice.

 
should tell the novice all the things they must know and be aware of to 


culture template / standard database inside the main git repo - for each field or experiment
has best practices built in, tells you what to check
shouldn't require a PR to change - maybe a contact form built in?

Contact information for certain culture templates are built in; experts in certain fields can 
 explicitly agree to have their contact info added to check stuff?


having a list of all contact info for every expert in a field seems rife for abuse - seperate personal table of emails 
voting
everyone must have an ORCiD?

besides the "expert list" (or maybe instead),
"user list" so breaking changes can be requested for comment - maybe even publications in the field?

where "user list" doesn't mean breaking changes

request for comment, "Voting" on changes


PR - set of standards to not mislead PR based on evidence


all people following the standard must use the latest version available at the beginning of work.
it is probably too much to ask for updates while performing the experiment,
but it should be acknowledged that from time to time invalidate 

if an expert can immediately identify problems in a study,
then it seems that some authors nowadays are operating without the same set of knowledge

can you trust the fourth estate

nist colloquium can we trust the estate


the process which we are following is itself, in a weird way, not scientific. problems 
keep happening.




The author believes that we have monumental problems looming at the horizon; it is fortunate that some part of these can be tackled by basic and advanced research, and our society has no shortage of monumental people bringing their efforts to bear. It would be a waste to be confused by trifles, or to even once repeat a mistake.

Science itself is founded on continuous improvement; as . This idea of continuous improvement, of "descending the gradient". 


thus, in a sense, software development, like aviation, is a "solved problem"; we can define a "solved problem" not when something has been demonstrated once, 
but when a process exists that can regularly produce correct results.

This is a living document. It is not a textbook from 1973 that everyone follows, yet has a typo on page 37 that you need to watch out for. 
We have that capability - this is *literally* what the internet was made for.

1. Make sure everyone can easily learn the things they need to know to get a good result in a field.  is operating from the same set of information.

Not all software developers follow these guidelines, perhaps because most software is unimportant, and the cost of implementing and following these guidelines would be too high. The software development community does not usually self-enforce any kind of software quality mandate. Software developers are not usually taxpayer funded.

On the other hand, things that matter are mandated; PCI guidelines for credit cards, 

But most of published science is now "safety-critical" in some sense or another. How can any journalist, any layman, anyone without subject-matter expertise possibly be expected to distinguish the basis for the following reports?

- **Fungi on Mars? Evidence of Growth and Behavior From Sequential Images**, by published in the peer-reviewed Journal of Cosmology and Advances in Microbiology, initially reported on credulously with some variation of "Scientists Find Evidence of Fungus Growing on Mars"
- "The Safety of COVID-19 Vaccinations—We Should Rethink the Policy", by several scientists of superficially reasonable pedigree, published in the peer-reviewed journal MDPI
- ""

Oh, and guess which ones you can read for free? That's right, *the ones that will kill you*.


We are told , but the consensus isn't anywhere that you can read. It exists only in the minds of people, and you can hardly ever know which people those are.

Is standardization going to help? Almost certainly not. I guess my hope is that, if papers must detail their violations of best practice, the poor-quality science will automatically undermine itself.


man, 
https://github.com/openjournals/joss-reviews/issues/1349
the JOSS people have got it figured out! Almost every part is automated,
there's a strict checklist,
and 



when doing software, it is very difficult to ensure exact implementation following a paper
unless a complete numerical example is provided for verification and validation


for the exosome paper, how did the standards publication affect things?
2720 papers cite that paper (gscholar). 
978 contain the text "materials and methods" (I.E. are probably experimental papers) non eof those have been retracted or have EOCs.

since 2018, 17,000 papers containing the text extracellular vesicles with the same m&m text.
23 of these have been retracted; 14 expressions of concern.
most of these are misconduct that a standard would not have helped with.






> The number of published articles per year increases exponentially, with “no indications that the growth rate has decreased in the last 50 years” (Larsen & Von Ins, 2010). For example, a search in ScienceDirect reveals that the number of published papers is 493545, 526380, 555342, and 619130 in 2010, 2011, 2012, and 2013, respectively. How long could such exponential growth be sustainable? It may lead to incredible burden on reviewers and editors, or an inevitable decline of research quality.

RE: RE: Publishing everything is fundamentally suboptimal for scientific progress https://journals.plos.org/plosone/article/comment?id=info:doi/10.1371/annotation/fdce24bb-aa1d-41ff-8325-a01fa8d97015

> but this is definitely not enough to cope with the main problem from which the scientific literature has been suffering in the last few decades, that is, the exponential growth of scientific publications driven by the obligation for scientists in many countries to publish more and more, which leads to the publication of a huge background noise of useless and low-level articles. It is indeed a tedious work to extract from the mass of submissions the scientific research that does merit the dissemination of its results.
> 
> As editors, we are committed since many years to our journal and will continue to invest much of our time to work with authors to promote nanoscience and nanotechnology in our journal by applying the highest standards thanks to the dedicated work of our associate editors and reviewers, but it is also clear that under the present circumstances, it is rarely rewarding and fun.
Pinna et al 10.1007/s11051-020-05094-0

According to the Retraction Watch database, there have been 461 previous papers with "False/Forged Authorship", 71 in Springer alone. With absolute respect to the experts at Springer integrity group, these are systemic problems, they have systemic solutions, and they aren't being fixed. It is vaguely disingenuous to suggest that no warning was given.
Unlike plagarism, 
Must be commended for conducting their investigation - but the specific changes made will probably not propagate throughout competing journals for years.

Authentication is a bare-minimum 

It is possible to 



I think all scientists would be able to contribute meaningfully to if they knew what was expected of them. We need the contributions of the "poor scientists".

(adversarial. most retractions seem to involve research misconduct)


